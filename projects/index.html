<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>projects | Zhang Yanting</title> <meta name="author" content="张 艳 婷"> <meta name="description" content="research interest and full paper."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/al-folio/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/al-folio/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/al-folio/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/al-folio/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jellyshuang.github.io/al-folio/projects/"> <link rel="stylesheet" href="/al-folio/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/al-folio/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/al-folio/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/al-folio/">Zhang Yanting</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/al-folio/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/publications/">publications</a> </li> <li class="nav-item active"> <a class="nav-link" href="/al-folio/projects/">projects<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/teaching/">teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/members/">members</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/al-folio/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/al-folio/projects/">projects</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">projects</h1> <p class="post-description">research interest and full paper.</p> </header> <article> <div class="projects"> <h2 class="category">research interests</h2> <div class="grid"> <div class="grid-sizer"></div> <div class="grid-item"> <a href="/al-folio/projects/1_project/"> <div class="card hoverable"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/iccvw-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/iccvw-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/iccvw-1400.webp"></source> <img src="/al-folio/assets/img/iccvw.jpg" width="auto" height="auto" alt="project thumbnail" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="card-body"> <h2 class="card-title text-lowercase">AI Traffic</h2> <p class="card-text">Multi-object tracking (MOT), Cross-camera tracking</p> <div class="row ml-1 mr-1 p-0"> </div> </div> </div> </a> </div> <div class="grid-sizer"></div> <div class="grid-item"> <a href="/al-folio/projects/2_project/"> <div class="card hoverable"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/publication_preview/dffFashion-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/publication_preview/dffFashion-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/publication_preview/dffFashion-1400.webp"></source> <img src="/al-folio/assets/img/publication_preview/dffFashion.jpg" width="auto" height="auto" alt="project thumbnail" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="card-body"> <h2 class="card-title text-lowercase">AI Fashion</h2> <p class="card-text">Diffusion-based fashion design</p> <div class="row ml-1 mr-1 p-0"> </div> </div> </div> </a> </div> <div class="grid-sizer"></div> <div class="grid-item"> <a href="https://unsplash.com" rel="external nofollow noopener" target="_blank"> <div class="card hoverable"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/tfy-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/tfy-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/tfy-1400.webp"></source> <img src="/al-folio/assets/img/tfy.jpg" width="auto" height="auto" alt="project thumbnail" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="card-body"> <h2 class="card-title text-lowercase">AI Sport</h2> <p class="card-text">Action quality assessment, Key event detection</p> <div class="row ml-1 mr-1 p-0"> </div> </div> </div> </a> </div> </div> <h2 class="category">full paper</h2> <div class="grid"> </div> </div> <div class="section" id="full-paper"> <h2>2023</h2> <div class="paper"> <ul> <td> <b>1. Zhang Y, Wang S, Fan Y, et al. TransLink: Transformer-Based Embedding for Tracklets’ Global Link[C]//ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023: 1-5..</b> <br> <a href="https://ieeexplore.ieee.org/abstract/document/10097136/" rel="external nofollow noopener" target="_blank">[Paper]</a> <br> </td> <td> <b>32. InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions</b> <br><a href="https://arxiv.org/pdf/2211.05778.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/OpenGVLab/InternImage" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/CVPR_2023_InternImage.txt">[BibTex]</a> <br> <strong>Wenhai Wang*</strong>, Jifeng Dai*, Zhe Chen*†, Zhenhang Huang*, Zhiqi Li*†, Xizhou Zhu*, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao# <br>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. <alert>(Highlight Paper (2.5%))</alert> <br><br> </td> <td> <b>31. Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks</b> <br><a href="https://arxiv.org/abs/2211.09808" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/fundamentalvision/Uni-Perceiver" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/CVPR_2023_UniPv2.txt">[BibTex]</a> <br> Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, <strong>Wenhai Wang</strong>, Jifeng Dai <br>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. <alert>(Highlight Paper (2.5%))</alert> <br><br> </td> <td> <b>30. Goal-oriented Autonomous Driving</b> <br><a href="https://arxiv.org/abs/2212.10156" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/OpenDriveLab/UniAD" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/CVPR_2023_UniAD.txt">[BibTex]</a> <br> Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, <strong>Wenhai Wang</strong>, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, Hongyang Li <br>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. <alert>(Best Paper Award)</alert> <br><br> </td> <td> <b>29. Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers</b> <br><a href="https://arxiv.org/pdf/2108.06932.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/DengPingFan/Polyp-PVT" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/CAAI_AIR_2023_Polyp_PVT.txt">[BibTex]</a> <br>Bo Dong, <strong>Wenhai Wang</strong>, Deng-Ping Fan#, Jinpeng Li, Huazhu Fu, Ling Shao <br>CAAI Artificial Intelligence Research (CAAI AIR), 2023&lt;/alert&gt; <br><br> </td> <td> <b>28. Vision Transformer Adapter for Dense Predictions</b> <br><a href="https://arxiv.org/pdf/2205.08534.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/czczup/ViT-Adapter" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/ICLR_2023_ViT_Adapter.txt">[BibTex]</a> <br>Zhe Chen*†, Yuchen Duan*†, <strong>Wenhai Wang#</strong>, Junjun He, Tong Lu, Jifeng Dai, Yu Qiao <br>International Conference on Learning Representations (ICLR), 2023. <alert>(Spotlight Paper (8.0%))</alert> <br><br> </td> </ul> </div> </div> <div class="section"> <h2>2022</h2> <div class="paper"> <ul> <td> <b>27. Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs</b> <br><a href="https://arxiv.org/pdf/2206.04674.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/fundamentalvision/Uni-Perceiver" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/NeurIPS_2022_UniP_MoE.txt">[BibTex]</a> <br>Jinguo Zhu, Xizhou Zhu, <strong>Wenhai Wang</strong>, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, Jifeng Dai# <br>Neural Information Processing Systems (NeurIPS), 2022. <br><br> </td> <td> <b>26. On efficient reinforcement learning for full-length game of StarCraft II</b> <br><a href="">[Paper]</a> <a href="">[Code]</a> <a href="">[BibTex]</a> <br>Ruoze Liu, Zhenjia Pang, Zhouyu Meng, <strong>Wenhai Wang</strong>, Yang Yu#, Tong Lu# <br>Journal of Artificial Intelligence Research (JAIR), 2022. <br><br> </td> <td> <b>25. BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers</b> <br><a href="https://arxiv.org/pdf/2203.17270.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/fundamentalvision/BEVFormer" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/ECCV_2022_BEVFormer.txt">[BibTex]</a> <a href="https://www.paperdigest.org/2023/01/most-influential-eccv-papers-2023-01/" rel="external nofollow noopener" target="_blank">[ECCV 2022' Top-10 Influential Papers]</a> <a href="https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022" rel="external nofollow noopener" target="_blank">[100 Most Cited AI Papers in 2022]</a> <br>Zhiqi Li*†, <strong>Wenhai Wang*</strong>, Hongyang Li*, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, Jifeng Dai# <br>European Conference on Computer Vision (ECCV), 2022. <br><br> </td> <td> <b>24. VL-LTR: Learning Class-wise Visual-Linguistic Representation for Long-Tailed Visual Recognition</b> <br><a href="https://arxiv.org/pdf/2111.13579.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/ChangyaoTian/VL-LTR" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/ECCV_2022_VL_LTR.txt">[BibTex]</a> <br>Changyao Tian*†, <strong>Wenhai Wang*</strong>, Xizhou Zhu*, Xiaogang Wang, Jifeng Dai#, Yu Qiao <br>European Conference on Computer Vision (ECCV), 2022. <br><br> </td> <td> <b>23. Incremental Few-Shot Semantic Segmentation via Embedding Adaptive-Update and Hyper-class Representation</b> <br><a href="https://arxiv.org/pdf/2207.12964.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="../bibtex/ACM_MM_2022_EHNET.txt">[BibTex]</a> <br>Guangchen Shi, Yirui Wu#, Jun Liu, Shaohua Wan, <strong>Wenhai Wang</strong>, Tong Lu <br>ACM International Conference on Multimedia (ACM MM), 2022. <br><br> </td> <td> <b>22. Generalized Focal Loss: Towards Efficient Representation Learning for Dense Object Detection</b> <br><a href="https://ieeexplore.ieee.org/document/9792391" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/implus/GFocal" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/TPAMI_2022_GFocal.txt">[BibTex]</a> <br>Xiang Li, Chengqi Lv, <strong>Wenhai Wang</strong>, Gang Li, Lingfeng Yang, Jian Yang# <br>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022. <alert>(ESI Highly Cited Paper (1%))</alert> <br><br> </td> <td> <b>21. Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers</b> <br><a href="https://arxiv.org/pdf/2109.03814.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/zhiqi-li/Panoptic-SegFormer" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/CVPR_2022_Panoptic_Segformer.txt">[BibTex]</a> <br>Zhiqi Li, <strong>Wenhai Wang#</strong>, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Tong Lu#, Ping Luo <br>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <br><br> </td> <td> <b>20. PVT v2: Improved Baselines with Pyramid Vision Transformer</b> <br><a href="https://link.springer.com/article/10.1007/s41095-022-0274-8" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/whai362/PVT" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="https://zhuanlan.zhihu.com/p/353222035" rel="external nofollow noopener" target="_blank">[中文解读]</a> <a href="../reports/wangwenhai_vision_transformer.pdf">[Report]</a> <a href="https://www.techbeat.net/talk-info?id=562" rel="external nofollow noopener" target="_blank">[Talk]</a> <a href="../bibtex/CVMJ_2021_PVTv2.txt">[BibTex]</a> <br><strong>Wenhai Wang#</strong>, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao <br>Computational Visual Media Journal (CVMJ), 2022. <alert>(ESI Highly Cited Paper (1%), ESI Hot Paper (0.1%), Best Paper Honorable Mention Award)</alert> <br><br> </td> <td> <b>19. Towards Ultra-Resolution Neural Style Transfer via Thumbnail Instance Normalization</b> <br><a href="https://arxiv.org/pdf/2103.11784.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/czczup/URST" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/AAAI_2022_URST.txt">[BibTex]</a> <br>Zhe Chen†, <strong>Wenhai Wang#</strong>, Enze Xie, Tong Lu#, Ping Luo <br>Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI), 2022. <br><br> </td> </ul> </div> </div> <div class="section"> <h2>2021</h2> <div class="paper"> <ul> <td> <b>18. Grid Dividing for Single-Stage Instance Segmentation</b> <br><strong>Wenhai Wang</strong>, Zhiqi Li, Tong Lu# <br>Journal of Software (JoS), 2021 (in Chinese). <br><br> </td> <td> <b>17. SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</b> <br><a href="https://proceedings.neurips.cc/paper/2021/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/NVlabs/SegFormer" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="https://zhuanlan.zhihu.com/p/379054782" rel="external nofollow noopener" target="_blank">[中文解读]</a> <a href="https://www.bilibili.com/video/BV1MV41147Ko/" rel="external nofollow noopener" target="_blank">[Demo]</a> <a href="../bibtex/NeurIPS_2021_SegFormer.txt">[BibTex]</a> <a href="https://www.paperdigest.org/2022/02/most-influential-nips-papers-2022-02/" rel="external nofollow noopener" target="_blank">[NeurIPS21' Top-10 Influential Papers]</a> <br>Enze Xie, <strong>Wenhai Wang</strong>, Zhiding Yu#, Anima Anandkuma, Jose M. Alvarez, Ping Luo# <br>Neural Information Processing Systems (NeurIPS), 2021. <br><br> </td> <td> <b>16. Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</b> <br><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/whai362/PVT" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../reports/PVT_Chinese.pdf">[中译版]</a> <a href="https://zhuanlan.zhihu.com/p/353222035" rel="external nofollow noopener" target="_blank">[中文解读]</a> <a href="../reports/wangwenhai_vision_transformer.pdf">[Report]</a> <a href="https://www.techbeat.net/talk-info?id=562" rel="external nofollow noopener" target="_blank">[Talk]</a> <a href="../bibtex/ICCV_2021_PVT.txt">[BibTex]</a> <a href="https://www.paperdigest.org/2022/02/most-influential-iccv-papers-2022-02/" rel="external nofollow noopener" target="_blank">[ICCV21' Top-10 Influential Papers]</a> <br><strong>Wenhai Wang</strong>, Enze Xie, Xiang Li, Deng-Ping Fan#, Kaitao Song, Ding Liang, Tong Lu#, Ping Luo, Ling Shao <br> IEEE/CVF International Conference on Computer Vision (ICCV), 2021. <alert>(Oral Presentation (3.4%), 2023 World Artificial Intelligence Conference Youth Outstanding Paper Award)</alert> <br><br> </td> <td> <b>15. DetCo: Unsupervised Contrastive Learning for Object Detection</b> <br><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Xie_DetCo_Unsupervised_Contrastive_Learning_for_Object_Detection_ICCV_2021_paper.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="../bibtex/ICCV_2021_DetCo.txt">[BibTex]</a> <br>Enze Xie*, Jian Ding*, <strong>Wenhai Wang</strong>, Xiaohang Zhan, Hang Xu, Zhenguo Li, Ping Luo# <br>IEEE/CVF International Conference on Computer Vision (ICCV), 2021. <br><br> </td> <td> <b>14. PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text</b> <br><a href="https://ieeexplore.ieee.org/document/9423611" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/whai362/pan_pp.pytorch" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/TPAMI_2021_PAN++.txt">[BibTex]</a> <br><strong>Wenhai Wang*</strong>, Enze Xie*, Xiang Li, Xuebo Liu, Ding Liang, Zhibo Yang, Tong Lu#, Chunhua Shen <br>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021. <br><br> </td> <td> <b>13. PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond</b> <br><a href="https://ieeexplore.ieee.org/document/9431650" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/xieenze/PolarMask" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/TPAMI_2021_PolarMask++.txt">[BibTex]</a> <br>Enze Xie*, <strong>Wenhai Wang*</strong>, Mingyu Ding, Ruimao Zhang, Ping Luo# <br>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021. <br><br> </td> <td> <b>12. Segmenting Transparent Object in the Wild with Transformer</b> <br><a href="https://www.ijcai.org/proceedings/2021/0165.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="../bibtex/IJCAI_2021_Trans2Seg.txt">[BibTex]</a> <br>Enze Xie, Wenjia Wang, <strong>Wenhai Wang</strong>, Peize Sun, Hang Xu, Ding Liang, Ping Luo# <br>International Joint Conference on Artificial Intelligence (IJCAI), 2021. <br><br> </td> <td> <b>11. Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</b> <br><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Generalized_Focal_Loss_V2_Learning_Reliable_Localization_Quality_Estimation_for_CVPR_2021_paper.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/implus/GFocalV2" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/CVPR_2021_GFLv2.txt">[BibTex]</a> <br>Xiang Li, <strong>Wenhai Wang</strong>, Xiaolin Hu, Jun Li, Jinhui Tang, Jian Yang# <br>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <br><br> </td> </ul> </div> </div> <div class="section"> <h2>2018-2020</h2> <div class="paper"> <ul> <td> <b>10. Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</b> <br><a href="https://proceedings.neurips.cc/paper/2020/file/f0bda020d2470f2e74990a07a607ebd9-Paper.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/implus/GFocal" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/NeurIPS_2020_GFocal.txt">[BibTex]</a> <br>Xiang Li, <strong>Wenhai Wang</strong>, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, Jian Yang# <br>Neural Information Processing Systems (NeurIPS), 2020. <br><br> <b>9. AE TextSpotter: Learning Visual and Linguistic Representation for Ambiguous Text Spotting</b> <br><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590443.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/whai362/TDA-ReCTS" rel="external nofollow noopener" target="_blank">[Dataset]</a> <a href="https://github.com/whai362/AE_TextSpotter" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/ECCV_2020_AE_TextSpotter.txt">[BibTex]</a> <br><strong>Wenhai Wang</strong>, Xuebo Liu, Xiaozhong Ji, Enze Xie, Ding Liang, ZhiBo Yang, Tong Lu#, Chunhua Shen, Ping Luo <br>European Conference on Computer Vision (ECCV), 2020. <br><br> <b>8. Segmenting Transparent Objects in the Wild</b> <br><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580681.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://xieenze.github.io/projects/TransLAB/TransLAB.html" rel="external nofollow noopener" target="_blank">[Dataset]</a> <a href="../bibtex/ECCV_2020_TransLab.txt">[BibTex]</a> <br>Enze Xie, Wenjia Wang, <strong>Wenhai Wang</strong>, Mingyu Ding, Chunhua Shen, Ping Luo# <br>European Conference on Computer Vision (ECCV), 2020. <br><br> <b>7. Differentiable Hierarchical Graph Grouping for Multi-Person Pose Estimation</b> <br><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520698.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="../bibtex/ECCV_2020_Differentiable.txt">[BibTex]</a> <br>Sheng Jin, Wentao Liu, Enze Xie, <strong>Wenhai Wang</strong>, Chen Qian, Wanli Ouyang, Ping Luo# <br>European Conference on Computer Vision (ECCV), 2020. <br><br> <b>6. Scene Text Image Super-Resolution in the Wild</b> <br><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550647.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="../bibtex/ECCV_2020_TextSR.txt">[BibTex]</a> <br>Wenjia Wang*, Enze Xie*, Xuebo Liu, <strong>Wenhai Wang</strong>, Ding Liang, Xiang Bai, Chunhua Shen <br>European Conference on Computer Vision (ECCV), 2020. <br><br> <b>5. PolarMask: Single Shot Instance Segmentation with Polar Representation</b> <br><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_PolarMask_Single_Shot_Instance_Segmentation_With_Polar_Representation_CVPR_2020_paper.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/xieenze/PolarMask" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="https://zhuanlan.zhihu.com/p/84890413" rel="external nofollow noopener" target="_blank">[中文解读]</a> <a href="https://www.bilibili.com/video/BV1dp4y1C7Ee?from=search&amp;seid=7560478987246751367" rel="external nofollow noopener" target="_blank">[Talk]</a> <a href="../bibtex/CVPR_2020_PolarMask.txt">[BibTex]</a> <a href="https://www.paperdigest.org/2021/02/most-influential-cvpr-papers/" rel="external nofollow noopener" target="_blank">[CVPR20' Top-10 Influential Papers]</a> <br>Enze Xie*, Peize Sun*, Xiaoge Song*, <strong>Wenhai Wang</strong>, Chunhua Shen, Ping Luo# <br> <br>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. <alert>(Oral Presentation (5.7%))</alert> <br><br> </td> <td> <b>4. Efficient and Accurate Arbitrary-Shaped Text Detection with Pixel Aggregation Network</b> <br><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Efficient_and_Accurate_Arbitrary-Shaped_Text_Detection_With_Pixel_Aggregation_Network_ICCV_2019_paper.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="../posters/ICCV_2019_PAN.pdf">[Poster]</a> <a href="https://github.com/whai362/pan_pp.pytorch" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/ICCV_2019_PAN.txt">[BibTex]</a> <br><strong>Wenhai Wang*</strong>, Enze Xie*, Xiaoge Song, Yuhang Zang, Wenjia Wang, Tong Lu#, Gang Yu, Chunhua Shen <br>IEEE/CVF International Conference on Computer Vision (ICCV), 2019. <br><br> <b>3. Shape Robust Text Detection with Progressive Scale Expansion Network</b> <br><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Shape_Robust_Text_Detection_With_Progressive_Scale_Expansion_Network_CVPR_2019_paper.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="../posters/CVPR_2019_PSENet.pdf">[Poster]</a> <a href="https://github.com/whai362/PSENet" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/CVPR_2019_PSENet.txt">[BibTex]</a> <br><strong>Wenhai Wang*</strong>, Enze Xie*, Xiang Li*, Wenbo Hou, Tong Lu#, Gang Yu, Shuai Shao <br>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. <br><br> <b>2. Selective Kernel Networks</b> <br><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Selective_Kernel_Networks_CVPR_2019_paper.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/implus/SKNet" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/CVPR_2019_SKNet.txt">[BibTex]</a> <br>Xiang Li, <strong>Wenhai Wang</strong>, Xiaolin Hu, Jian Yang# <br>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. <br><br> <b>1. Mixed Link Networks</b> <br><a href="https://www.ijcai.org/Proceedings/2018/0391.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="../posters/IJCAI_2018_MixNet.pdf">[Poster]</a> <a href="https://github.com/DeepInsight-PCALab/MixNet" rel="external nofollow noopener" target="_blank">[Code]</a> <a href="../bibtex/IJCAI_2018_MixNet.txt">[BibTex]</a> <br><strong>Wenhai Wang*</strong>, Xiang Li*, Jian Yang#, Tong Lu# <br> International Joint Conference on Artificial Intelligence (IJCAI), 2018. <alert>(Oral Presentation)</alert> <br><br> </td> </ul> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 张 艳 婷. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/al-folio/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/al-folio/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/al-folio/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/al-folio/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/al-folio/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>